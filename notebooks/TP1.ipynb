{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1: Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trabajo práctico 1 de la materia se basa en el análisis de los tweets del set de datos de la competencia: https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "- id - identificador unico para cada  tweet\n",
    "- text - el texto del tweet\n",
    "- location - ubicación desde donde fue enviado (podría no estar)\n",
    "- keyword - un keyword para el tweet  (podría faltar)\n",
    "- target - en train.csv, indica si se trata de un desastre real  (1) o no (0)\n",
    "\n",
    "El objetivo del primer TP es realizar un análisis exploratorio del set de datos. Queremos ver qué cosas podemos descubrir sobre los datos que puedan resultar interesantes. Estas cosas pueden estar relacionadas al objetivo del TP2 (predecir si un cierto tweet es real o no) o no, ambas son de interés.\n",
    "\n",
    "Los requisitos de la primera entrega son los siguientes:\n",
    "\n",
    "- El análisis debe estar hecho en Python Pandas o R.\n",
    "- El análisis debe entregarse en formato pdf vía gradescope. En el informe no va código.\n",
    "- Informar el link a un repositorio Github en donde pueda bajarse el código completo para generar el análisis.\n",
    "\n",
    "La evaluación del TP se realizará en base al siguiente criterio:\n",
    "\n",
    "- Originalidad del análisis exploratorio. \n",
    "- Calidad del reporte. ¿Está bien escrito? ¿Es claro y preciso? \n",
    "- Calidad del análisis exploratorio: qué tipo de preguntas se hacen y de qué forma se responden, ¿es la respuesta clara y concisa con respecto a la pregunta formulada? \n",
    "- Calidad de las visualizaciones presentadas.\n",
    "  - ¿Tienen todos los ejes su rótulo?\n",
    "  - ¿Tiene cada visualización un título?\n",
    "  - ¿Es entendible la visualización sin tener que leer la explicación?\n",
    "  - ¿El tipo de plot elegido es adecuado para lo que se quiere visualizar?\n",
    "  - ¿Es una visualización interesante?\n",
    "  - ¿El uso del color es adecuado?\n",
    "  - ¿Hay un exceso o falta de elementos visuales en la visualización elegida?\n",
    "  - ¿La visualización es consistente con los datos?\n",
    "- Conclusiones presentadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE acá estamos reemplazando todos los NaN por None\n",
    "tweets = tweets.fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de la columna text y creación de nuevas columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera normalización del texto, pasamos a lowercase \n",
    "tweets['normalized_text'] = tweets.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los links del texto y dejandolos en una columna a parte por si sirven a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nueva columna con el total de links\n",
    "import re\n",
    "URLPATTERN = r'(https?://\\S+)' \n",
    "\n",
    "tweets['urls'] = tweets.normalized_text.apply(lambda x: re.findall(URLPATTERN, x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(URLPATTERN,\"\", x))\n",
    "# cuento la cantidad de links en los tweets\n",
    "tweets['url_count'] = tweets.urls.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['url_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los hashtags del texto y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de hashtags en los tweets\n",
    "# nueva columna con el total de hashtags, y los hashtags\n",
    "tweets['hashtags'] = tweets.normalized_text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "tweets['hashtags_count'] = tweets.hashtags.str.len()\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"#(\\w+)\",\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['hashtags_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los tags del texto y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de ags en los tweets\n",
    "# nueva columna con el total de tags, y los tags\n",
    "tweets['tags'] = tweets.text.str.lower().apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"@(\\w+)\",\"\", x))\n",
    "tweets['tags_count'] = tweets.tags.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['tags_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos signos de puntuación y html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuation(text):        \n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlsymbols(text):\n",
    "    soup = BeautifulSoup(unescape(text))\n",
    "    return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_htmlsymbols)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_punctuation)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_emojis_non_ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos stop words y creamos nueva columna con array de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el texto en listado de palabras y despues borramos las stop words\n",
    "tweets['words'] = tweets.normalized_text.str.split()\n",
    "stop_words = stopwords.words('english')\n",
    "tweets['normalized_words'] = tweets['words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# eliminar las stop words del texto normalizado\n",
    "tweets['normalized_text'] = [' '.join(map(str, l)) for l in tweets['normalized_words']]\n",
    "# vemos como queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizamos si el texto tiene números y guardamos el dato en una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto chequea números como 1,2 y tambíen escrito como one, two\n",
    "def existence_of_numeric_data(text):\n",
    "    text=nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    count = 0\n",
    "    for i in range(len(pos)):\n",
    "        word , pos_tag = pos[i]\n",
    "        if pos_tag == 'CD':\n",
    "            return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con 1 si tiene , 0 sino\n",
    "tweets['has_numbers'] = tweets.normalized_text.apply(existence_of_numeric_data)\n",
    "# vemos como queda el dataset\n",
    "tweets[(tweets['has_numbers'] == 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de la columna keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos \"%20\" que representa espacio\n",
    "tweets['keywords'] = tweets.keyword.str.replace('%20',' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
