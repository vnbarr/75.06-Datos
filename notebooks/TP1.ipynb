{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1: Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El trabajo práctico 1 de la materia se basa en el análisis de los tweets del set de datos de la competencia: https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "- id - identificador unico para cada  tweet\n",
    "- text - el texto del tweet\n",
    "- location - ubicación desde donde fue enviado (podría no estar)\n",
    "- keyword - un keyword para el tweet  (podría faltar)\n",
    "- target - en train.csv, indica si se trata de un desastre real  (1) o no (0)\n",
    "\n",
    "El objetivo del primer TP es realizar un análisis exploratorio del set de datos. Queremos ver qué cosas podemos descubrir sobre los datos que puedan resultar interesantes. Estas cosas pueden estar relacionadas al objetivo del TP2 (predecir si un cierto tweet es real o no) o no, ambas son de interés.\n",
    "\n",
    "Los requisitos de la primera entrega son los siguientes:\n",
    "\n",
    "- El análisis debe estar hecho en Python Pandas o R.\n",
    "- El análisis debe entregarse en formato pdf vía gradescope. En el informe no va código.\n",
    "- Informar el link a un repositorio Github en donde pueda bajarse el código completo para generar el análisis.\n",
    "\n",
    "La evaluación del TP se realizará en base al siguiente criterio:\n",
    "\n",
    "- Originalidad del análisis exploratorio. \n",
    "- Calidad del reporte. ¿Está bien escrito? ¿Es claro y preciso? \n",
    "- Calidad del análisis exploratorio: qué tipo de preguntas se hacen y de qué forma se responden, ¿es la respuesta clara y concisa con respecto a la pregunta formulada? \n",
    "- Calidad de las visualizaciones presentadas.\n",
    "  - ¿Tienen todos los ejes su rótulo?\n",
    "  - ¿Tiene cada visualización un título?\n",
    "  - ¿Es entendible la visualización sin tener que leer la explicación?\n",
    "  - ¿El tipo de plot elegido es adecuado para lo que se quiere visualizar?\n",
    "  - ¿Es una visualización interesante?\n",
    "  - ¿El uso del color es adecuado?\n",
    "  - ¿Hay un exceso o falta de elementos visuales en la visualización elegida?\n",
    "  - ¿La visualización es consistente con los datos?\n",
    "- Conclusiones presentadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword tiene 61 nulls\n",
    "# location tiene 2533 nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.location.value_counts() # se podría unificar varios a USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de los falsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 0][\"text\"] #los falsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera normalización del texto, pasamos a lowercase \n",
    "tweets['normalized_text'] = tweets.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los links del texto y dejandolos en una columna a parte por si sirven a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nueva columna con el total de links\n",
    "import re\n",
    "URLPATTERN = r'(https?://\\S+)' \n",
    "\n",
    "tweets['urls'] = tweets.normalized_text.apply(lambda x: re.findall(URLPATTERN, x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(URLPATTERN,\"\", x))\n",
    "# cuento la cantidad de links en los tweets\n",
    "tweets['url_count'] = tweets.urls.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 1].keyword.value_counts() #los posta, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['url_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminando palabras que no son super importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltkfrom nltk.corpus \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de hashtags en los tweets\n",
    "# nueva columna con el total de hashtags, y los hashtags\n",
    "tweets['hashtags'] = tweets.normalized_text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "tweets['hashtags_count'] = tweets.hashtags.str.len()\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"#(\\w+)\",\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['hashtags_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los tags del texto y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de ags en los tweets\n",
    "# nueva columna con el total de tags, y los tags\n",
    "tweets['tags'] = tweets.text.str.lower().apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"@(\\w+)\",\"\", x))\n",
    "tweets['tags_count'] = tweets.tags.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos como queda el dataset\n",
    "tweets[(tweets['tags_count'] > 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contador de links (DESCARTADO)\n",
    "Quiero ver si hay relación entre cantidad de tweets fakes y no fakes y la cantidad de links (DESCARTADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de links en los tweets\n",
    "import re\n",
    "URLPATTERN = r'(https?://\\S+)' \n",
    "# nueva columna con el total de links\n",
    "tweets['url_count'] = tweets.text.apply(lambda x: re.findall(URLPATTERN, x)).str.len()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 1].url_count.value_counts() #los posta, cantidad de urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 0].url_count.value_counts() #los fake, cantidad de urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico esto para ver si hay relación\n",
    "g = sns.boxplot(x=\"target\", y=\"url_count\", \n",
    "                     data=tweets, palette=\"cubehelix\")\n",
    "g.set_title(\"Cantidad de links según veracidad\", fontsize=18)\n",
    "g.set_xlabel(\"Real?\", fontsize=14)\n",
    "g.set_ylabel(\"Cantidad de links\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuation(text):        \n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from html import unescape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlsymbols(text):\n",
    "    soup = BeautifulSoup(unescape(text))\n",
    "    return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_htmlsymbols)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_punctuation)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_emojis_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Eliminamos stop words y creamos nueva columna con array de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el texto en listado de palabras y despues borramos las stop words\n",
    "tweets['words'] = tweets.normalized_text.str.split()\n",
    "stop_words = stopwords.words('english')\n",
    "tweets['normalized_words'] = tweets['words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# eliminar las stop words del texto normalizado\n",
    "tweets['normalized_text'] = [' '.join(map(str, l)) for l in tweets['normalized_words']]\n",
    "# vemos como queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos los tags\n",
    "tweets.text.str.extractall(r'(\\@\\w+)')[0].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 1].tags_count.value_counts() #los posta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 0].tags_count.value_counts() #los fake}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafico esto para ver si hay relación\n",
    "g = sns.boxplot(x=\"target\", y=\"tags_count\", \n",
    "                     data=tweets, palette=\"cubehelix\")\n",
    "g.set_title(\"Cantidad de tags según veracidad\", fontsize=18)\n",
    "g.set_xlabel(\"Real?\", fontsize=14)\n",
    "g.set_ylabel(\"Cantidad de tags\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizamos si el texto tiene números y guardamos el dato en una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis sobre cuestiones vistas despues de la reunion con Vir.\n",
    "\n",
    "- Analisis de Tweets c/location vs Tweets s/location\n",
    "- Keywords / Palabras y veracidad\n",
    "- Relacion entre location - existencia en el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Analisis de Tweets c/location vs tweets s/location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cantidad de tweets con y sin location\n",
    "tweets['location'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tweets reales y no\n",
    "tweets['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#relacion entre ambos datos\n",
    "tweets['has_location'] = ~tweets['location'].isna()\n",
    "tweets[['has_location','target']].groupby(['target']).count()\n",
    "#no aporta nada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].str.replace('\\s','%20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"apariciones / total de tweets\" va a dar una aproximacion mejor de la ponderacion de las palabras sobre los tweets\n",
    "total_keyword = tweets['keywords'].value_counts().count()\n",
    "total_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets[\"target\"] == 0][\"keyword\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_false = tweets[tweets[\"target\"] == 0][\"keyword\"].value_counts()\n",
    "keyword_true  = tweets[tweets[\"target\"] == 1][\"keyword\"].value_counts()\n",
    "#keyword_false.index = keyword_false.index.str.replace('\\s','%20')\n",
    "keyword_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relacion entre keywords y verac\n",
    "\n",
    "g = sns.barplot(x=keyword_false.head(20).values,y=keyword_false.head(20).index.str.replace('%20',' '),color='red',orient=\"h\").set(title = 'Top 30 keywords (Tweets falsos)',xlabel='Apariciones')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_false.merge(keyword_real)\n",
    "f = sns.barplot(x=keyword_true.head(20).values,y=keyword_true.head(20).index,color='green',orient=\"h\").set(title = 'Top 30 keywords (Tweets verdaderos)',xlabel='Apariciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparacion entre datasets?. buscar palabras en tops. - pendiente comparacion entre dos datasets\n",
    "comparewords = pd.concat([keyword_true,keyword_false], axis=1)\n",
    "comparewords.columns=['valid','invalid']\n",
    "\n",
    "comparewords = comparewords.fillna(0)\n",
    "comparewords = comparewords.sort_values(by=['valid'])\n",
    "\n",
    "#comparewords_ds.plot.barh(rot=0)\n",
    "comparewords.tail(15).plot.barh(rot=0).set(title='descartar?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comparewords = comparewords.sort_values(by=['invalid'])\n",
    "comparewords.tail(15).plot.barh(rot=0).set(title='descartar?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relacion entre location y existencia en el texto.\n",
    "por lo visto es un dato irrelevante que se podria descartar. ya que la location puede ser cualquier cosa y no tiene nada que ver con el lugar de donde se tuitea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.fillna('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#casos excluidos (si no tienen location, obviamente no van a estar en el texto)\n",
    "tweets['text_in_location'] = tweets[['text','location']].apply(lambda row: 'Existe' if row.location in row.text else 'No existe' ,axis = 1)\n",
    "tweets['text_in_location'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_location = tweets['text_in_location'].value_counts()\n",
    "\n",
    "count_location.plot.bar(rot = 0).set(title = 'Correspondencia de location y su existencia en el texto',ylabel='cantidad tweets')\n",
    "#g = sns.boxplot(x=\"target\", y=\"url_count\", \n",
    "#                     data=tweets, palette=\"cubehelix\")\n",
    "#g.set_title(\"Cantidad de links según veracidad\", fontsize=18)\n",
    "#g.set_xlabel(\"Real?\", fontsize=14)\n",
    "#g.set_ylabel(\"Cantidad de links\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **No tiene mucho sentido hacer una clasificacion de tweets reales/falsos dentro de la correspondencia de location/texto , ya que es muy poca la relacion y no nos dice nada.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Graficos sobre series numericas.\n",
    "#tweets[tweets[\"target\"] == 1].select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pendiente\n",
    "tweets['url_len'] = tweets.urls.astype(str).map(len)\n",
    "tweets['text_len'] = tweets.text.map(len)\n",
    "\n",
    "ax = sns.kdeplot(tweets[tweets[\"target\"] == 0].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 0].url_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 0].text_len , \\\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax = sns.kdeplot( tweets[tweets[\"target\"] == 1].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 1].url_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 1].text_len, \\\n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pendiente\n",
    "tweets['hashtags_len'] = tweets.hashtags.astype(str).map(len)\n",
    "tweets['tags_len'] = tweets.tags.astype(str).map(len)\n",
    "tweets['words_len'] = tweets.words.astype(str).map(len)\n",
    "tweets['normalized_words_len'] = tweets.normalized_words.astype(str).map(len)\n",
    "\n",
    "ax = sns.kdeplot(tweets[tweets[\"target\"] == 0].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 0].hashtags_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 0].text_len , \\\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax = sns.kdeplot( tweets[tweets[\"target\"] == 1].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 1].hashtags_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 1].text_len, \\\n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(tweets[tweets[\"target\"] == 0].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 0].tags_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 0].text_len , \\\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax = sns.kdeplot( tweets[tweets[\"target\"] == 1].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 1].tags_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 1].text_len, \\\n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(tweets[tweets[\"target\"] == 0].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 0].words_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 0].text_len , \\\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax = sns.kdeplot( tweets[tweets[\"target\"] == 1].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 1].words_len*100/ \\\n",
    "                 tweets[tweets[\"target\"] == 1].text_len, \\\n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(tweets[tweets[\"target\"] == 0].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 0].normalized_words_len *100/ \\\n",
    "                 tweets[tweets[\"target\"] == 0].text_len , \\\n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax = sns.kdeplot( tweets[tweets[\"target\"] == 1].text_len,\\\n",
    "                 tweets[tweets[\"target\"] == 1].normalized_words_len*100/ \\\n",
    "                 tweets[tweets[\"target\"] == 1].text_len, \\\n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7)\n",
    "ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pendiente\n",
    "text_analytics = pd.DataFrame()\n",
    "\n",
    "text_analytics['url_len'] = tweets.urls.astype(str).map(len)\n",
    "text_analytics['text_len'] = tweets.text.map(len)\n",
    "text_analytics['hashtags_len'] = tweets.hashtags.astype(str).map(len)\n",
    "text_analytics['tags_len'] = tweets.tags.astype(str).map(len)\n",
    "text_analytics['words_len'] = tweets.words.astype(str).map(len)\n",
    "text_analytics['normalized_words_len'] = tweets.normalized_words.astype(str).map(len)\n",
    "text_analytics['target'] = tweets[['target']].apply(lambda row: row.target==1 ,axis = 1)\n",
    "\n",
    "gsnum = sns.pairplot(text_analytics,hue=\"target\",diag_kind=\"kde\").set(title='descartar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#g = sns.distplot(countUrl[['valid']], color=\"green\",label=\"valid\",hist=False,rug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analisis en base a la cantidad de caracters\n",
    "- lenght vs hashtag-\n",
    "- lenght vs url-\n",
    "- lenght vs tags-\n",
    "\n",
    "(ya lo vio Vir, en todo caso evaluar la visualizacion y si sirve el Distplot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de keywords - PARTE 2\n",
    "\n",
    "algunas palabras similares tienen distintos indices de aparicion. Analizo el coeficiente (valido e invalido) para las palabras ordenadas alfabeticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabla pivot para ver las apariciones(unicas) en cada tipo de tweet\n",
    "keyword_in_target = pd.pivot_table(\n",
    "tweets[['id', 'keyword', 'target']], \n",
    "index='keyword',columns='target',aggfunc=lambda x: len(x.unique()),fill_value=0\n",
    ")\n",
    "keyword_in_target.reset_index()\n",
    "#rename columnas\n",
    "keyword_in_target.columns= ['valid','invalid']\n",
    "#agrego columna total y calculo promedio\n",
    "keyword_in_target['total'] = keyword_in_target[['valid','invalid']].apply(lambda row: row.valid+ row.invalid ,axis = 1)\n",
    "\n",
    "def averageshow(args):\n",
    "    valid =args[0]/args[2]\n",
    "    invalid = args[1]/args[2] \n",
    "    return pd.Series([valid,invalid])\n",
    "\n",
    "a = keyword_in_target[['valid','invalid','total']].apply(averageshow,axis=1)\n",
    "a.reset_index()\n",
    "a.columns=['valid','invalid']\n",
    "a.head(10).plot.barh(rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tail(10).plot.barh(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA:\n",
    "podria armar algo similar con la columna Words y ver si hay algo interesante.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis sobre la columna location - Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_bar_true = tweets[tweets[\"target\"] == 0]['location'].value_counts().reset_index()\n",
    "location_bar_false = tweets[tweets[\"target\"] == 1]['location'].value_counts().reset_index()\n",
    "\n",
    "#location_bar_df.columns = ['location', 'count']\n",
    "location_bar = pd.concat([location_bar_true.set_index('index'), location_bar_false.set_index('index')], axis=1, join='outer')\n",
    "location_bar.columns = ['valid','invalid']\n",
    "location_bar.head(10).plot.barh(rot=0)\n",
    "location_bar.tail(10).plot.barh(rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**antes debo limpiar la data en Location**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizo la cantidad de palabras (token_len)  y lo analizo en relacion del largo del tweet (text_len\n",
    "Creo que daria el mismo resultado contando las palabras de Words o Normalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropna=False\n",
    "\n",
    "text_simple = pd.DataFrame()\n",
    "text_simple['cantidad_palabras'] = tweets['text'].apply(lambda x: len([token for token in x.split()]))\n",
    "text_simple['largo_texto'] = tweets['text'].apply(lambda x: len(x))\n",
    "text_simple['estado'] = tweets[['target']].apply(lambda row: row.target==1 ,axis = 1)\n",
    "\n",
    "g = sns.FacetGrid(text_simple, col=\"estado\",height=5)\n",
    "g = g.map(sns.distplot, \"largo_texto\").set(ylabel=\"densidad\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.FacetGrid(text_simple, col=\"estado\",height=5)\n",
    "g = g.map(sns.distplot, \"cantidad_palabras\").set(ylabel=\"densidad\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relacion entre cantidad de palabras y largo de texto\n",
    "ax = sns.kdeplot(text_simple[text_simple[\"estado\"]==False].cantidad_palabras,                  \n",
    "                 text_simple[text_simple[\"estado\"]==False].largo_texto, \n",
    "                 cmap=\"Reds\", shade=True, shade_lowest=False, alpha=0.7).set(title='Largo texto vs Cantidad de palabras (Tweets falsos)')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az = sns.kdeplot(text_simple[text_simple[\"estado\"]==True].cantidad_palabras,                  \n",
    "                 text_simple[text_simple[\"estado\"]==True].largo_texto, \n",
    "                 cmap=\"Greens\", shade=True, shade_lowest=False, alpha=0.7).set(title='Largo texto vs Cantidad de palabras (Tweets reales)')\n",
    "az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellchecker\n",
    "Es un corrector de escritura, se podria aplicar a los tweets para limpiar la data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Muy lento\n",
    "\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "#spell = SpellChecker()\n",
    "#def correct_spellings(text):\n",
    "#    corrected_text = []\n",
    "#    misspelled_words = spell.unknown(text.split())\n",
    "#    for word in text.split():\n",
    "#        if word in misspelled_words:\n",
    "#            corrected_text.append(spell.correction(word))\n",
    "#        else:\n",
    "#            corrected_text.append(word)\n",
    "#    return \" \".join(corrected_text)\n",
    " \n",
    "\n",
    "#correct_spellings('')\n",
    "\n",
    "#tweets['normalized_text']=tweets['normalized_text'].apply(lambda x : correct_spellings(x))\n",
    "\n",
    "#tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otras cosillas\n",
    "Esto en general va arriba. la distribucion de tweets falsos y verdaderos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extracting the number of examples of each class\n",
    "Real_len = tweets[tweets['target'] == 1].shape[0]\n",
    "Not_len = tweets[tweets['target'] == 0].shape[0]\n",
    "\n",
    "# bar plot of the 3 classes\n",
    "plt.rcParams['figure.figsize'] = (7, 5)\n",
    "plt.bar(10,Real_len,3, label=\"Real\", color='green')\n",
    "plt.bar(15,Not_len,3, label=\"Falso\", color='red')\n",
    "plt.legend()\n",
    "plt.ylabel('Numero de muestras')\n",
    "plt.title('Distribución de tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
