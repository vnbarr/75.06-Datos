{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo TP es una competencia de Machine Learning en donde cada grupo debe intentar determinar, para cada tweet brindado, si el mismo esta basado en un hecho real o no.\n",
    "\n",
    "La competencia se desarrolla en la plataforma de Kaggle  https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "El dataset consta de una serie de tweets, para los cuales se informa:\n",
    "\n",
    "- id - identificador unico para cada  tweet\n",
    "- text - el texto del tweet\n",
    "- location - ubicación desde donde fue enviado (podría no estar)\n",
    "- keyword - un keyword para el tweet  (podría faltar)\n",
    "- target - en train.csv, indica si se trata de un desastre real  (1) o no (0) \n",
    "\n",
    "Los submits con el resultado deben tener el formato:\n",
    "- Id: Un id numérico para identificar el tweet\n",
    "- target: 1 / 0 según se crea que el tweet se trata sobre un desastre real, o no.\n",
    "\n",
    "Los grupos deberán probar distintos algoritmos de Machine Learning para intentar predecir si el tweet está basado en hechos reales o no. A medida que los grupos realicen pruebas deben realizar el correspondiente submit en Kaggle para evaluar el resultado de los mismos.\n",
    "\n",
    "Al finalizar la competencia el grupo que mejor resultado tenga obtendrá 10 puntos para cada uno de sus integrantes que podrán ser usados en el examen por promoción o segundo recuperatorio.\n",
    "\n",
    "Requisitos para la entrega del TP2:\n",
    "\n",
    "- El TP debe programarse en Python o R.\n",
    "- Debe entregarse un pdf con el informe de algoritmos probados, algoritmo final utilizado, transformaciones realizadas a los datos, feature engineering, etc. \n",
    "- El informe debe incluir también un link a github con el informe presentado en pdf, y todo el código.\n",
    "- El grupo debe presentar el TP en una computadora en la fecha indicada por la cátedra, el TP debe correr en un lapso de tiempo razonable (inferior a 1 hora) y generar un submission válido que iguale el mejor resultado obtenido por el grupo en Kaggle. (mas detalles a definir)\n",
    "\n",
    "El TP2 se va a evaluar en función del siguiente criterio:\n",
    "\n",
    "- Cantidad de trabajo (esfuerzo) del grupo: ¿Probaron muchos algoritmos? ¿Hicieron un buen trabajo de pre-procesamiento de los datos y feature engineering?\n",
    "- Resultado obtenido en Kaggle (obviamente cuanto mejor resultado mejor nota)\n",
    "- Presentación final del informe, calidad de la redacción, uso de información obtenida en el TP1, conclusiones presentadas.\n",
    "- Performance de la solución final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías a usar\n",
    "# para escapear html\n",
    "!pip install bs4\n",
    "# para normalización del texto\n",
    "!pip install nltk\n",
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "#Para generar una paleta de colores equivalente a cubehelix(sns) en matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "#Nueva paleta de colores para matplotlib\n",
    "color_hex_arr = [\"#2B6F39\", \"#C2D8F2\",\"#182D48\",\"#A1784A\",\"#D38FC5\"]\n",
    "cubehelix_map = ListedColormap(sns.color_palette(color_hex_arr).as_hex())\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv', encoding='utf-8')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitamos la columna Location\n",
    "Del análisis del tp1 vimos que la columna location tenía muchos valores nulos y muchísimos valores únicos, muchas veces con textos sin sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['location'], axis=1)\n",
    "train.drop(['location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el daset tweets para después poder aplicarle todas las tranformaciones juntas, después volvemos a dividir\n",
    "tweets = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregado de nuevas columnas y limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera normalización del texto, pasamos a lowercase \n",
    "tweets['normalized_text'] = tweets.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización del texto y nuevas columnas a partir del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los links del texto normalizado y guardamos los links en una columna a parte por si sirven a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLPATTERN = r'(https?://\\S+)' \n",
    "\n",
    "tweets['urls'] = tweets.normalized_text.apply(lambda x: re.findall(URLPATTERN, x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(URLPATTERN,\"\", x))\n",
    "# cuento la cantidad de links en los tweets\n",
    "tweets['url_count'] = tweets.urls.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los hashtags del texto normalizado y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de hashtags en los tweets\n",
    "# nueva columna con el total de hashtags, y los hashtags\n",
    "tweets['hashtags'] = tweets.normalized_text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "tweets['hashtags_count'] = tweets.hashtags.str.len()\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"#(\\w+)\",\"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los tags del texto y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de ags en los tweets\n",
    "# nueva columna con el total de tags, y los tags\n",
    "tweets['tags'] = tweets.text.str.lower().apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"@(\\w+)\",\"\", x))\n",
    "tweets['tags_count'] = tweets.tags.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contamos cantidad de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentences_count'] = tweets.normalized_text.apply(lambda x : len(sent_tokenize(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos signos de puntuación y html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "# !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuation(text):        \n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlsymbols(text):\n",
    "    soup = BeautifulSoup(unescape(text))\n",
    "    return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_htmlsymbols)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_punctuation)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_emojis_non_ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos stop words y creamos nueva columna con array de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el texto en listado de palabras y despues borramos las stop words\n",
    "tweets['words'] = tweets.normalized_text.str.split()\n",
    "stop_words = stopwords.words('english')\n",
    "tweets['normalized_words'] = tweets['words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# eliminar las stop words del texto normalizado\n",
    "tweets['normalized_text'] = [' '.join(map(str, l)) for l in tweets['normalized_words']]\n",
    "# vemos como queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contador de palabras y frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos la cantidad de palabras de cada tweet\n",
    "tweets['words_counter'] = tweets.normalized_words.apply(Counter)\n",
    "# me quedo con las palabras que mas ocurrencias tienen en cada row\n",
    "tweets['word_max_appearance'] = tweets.words_counter.apply( lambda x: max(x) if x else None)  \n",
    "tweets['word_max_appearance_count'] =  tweets.words_counter.apply( lambda x: max(x.values()) if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizamos si el texto tiene números y guardamos el dato en una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto chequea números como 1,2 y tambíen escrito como one, two\n",
    "def existence_of_numeric_data(text):\n",
    "    text=nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    count = 0\n",
    "    for i in range(len(pos)):\n",
    "        word , pos_tag = pos[i]\n",
    "        if pos_tag == 'CD':\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con 1 si tiene , 0 sino\n",
    "tweets['has_numbers'] = tweets.normalized_text.apply(existence_of_numeric_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de la columna keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos \"%20\" que representa espacio\n",
    "tweets['keywords'] = tweets.keyword.str.replace('%20',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correccion tipo de datos en keyword para optimizacion de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['keyword']:\n",
    "    tweets[col] = tweets[col].astype('category')\n",
    "#check de tipos de datos categoricos \n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregado de columnas para Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a float for sentiment strength based on the input text. Positive values are positive valence, negative value are negative valence.\n",
    "tweets['sentiment_score'] = tweets.normalized_text.apply(lambda x: sid.polarity_scores(x))\n",
    "tweets['sentiment_score_compound'] = tweets.sentiment_score.apply(lambda x: x['compound'])\n",
    "tweets['sentiment_score_pos'] = tweets.sentiment_score.apply(lambda x: x['pos'])\n",
    "tweets['sentiment_score_neg'] = tweets.sentiment_score.apply(lambda x: x['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos cómo queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volvemos a separar los sets de datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([tweets.set_index('id'),test.set_index('id')], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([tweets.set_index('id'),train.set_index('id')], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {},
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
