{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Enunciado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo TP es una competencia de Machine Learning en donde cada grupo debe intentar determinar, para cada tweet brindado, si el mismo esta basado en un hecho real o no.\n",
    "\n",
    "La competencia se desarrolla en la plataforma de Kaggle  https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "El dataset consta de una serie de tweets, para los cuales se informa:\n",
    "\n",
    "- id - identificador unico para cada  tweet\n",
    "- text - el texto del tweet\n",
    "- location - ubicación desde donde fue enviado (podría no estar)\n",
    "- keyword - un keyword para el tweet  (podría faltar)\n",
    "- target - en train.csv, indica si se trata de un desastre real  (1) o no (0) \n",
    "\n",
    "Los submits con el resultado deben tener el formato:\n",
    "- Id: Un id numérico para identificar el tweet\n",
    "- target: 1 / 0 según se crea que el tweet se trata sobre un desastre real, o no.\n",
    "\n",
    "Los grupos deberán probar distintos algoritmos de Machine Learning para intentar predecir si el tweet está basado en hechos reales o no. A medida que los grupos realicen pruebas deben realizar el correspondiente submit en Kaggle para evaluar el resultado de los mismos.\n",
    "\n",
    "Al finalizar la competencia el grupo que mejor resultado tenga obtendrá 10 puntos para cada uno de sus integrantes que podrán ser usados en el examen por promoción o segundo recuperatorio.\n",
    "\n",
    "Requisitos para la entrega del TP2:\n",
    "\n",
    "- El TP debe programarse en Python o R.\n",
    "- Debe entregarse un pdf con el informe de algoritmos probados, algoritmo final utilizado, transformaciones realizadas a los datos, feature engineering, etc. \n",
    "- El informe debe incluir también un link a github con el informe presentado en pdf, y todo el código.\n",
    "- El grupo debe presentar el TP en una computadora en la fecha indicada por la cátedra, el TP debe correr en un lapso de tiempo razonable (inferior a 1 hora) y generar un submission válido que iguale el mejor resultado obtenido por el grupo en Kaggle. (mas detalles a definir)\n",
    "\n",
    "El TP2 se va a evaluar en función del siguiente criterio:\n",
    "\n",
    "- Cantidad de trabajo (esfuerzo) del grupo: ¿Probaron muchos algoritmos? ¿Hicieron un buen trabajo de pre-procesamiento de los datos y feature engineering?\n",
    "- Resultado obtenido en Kaggle (obviamente cuanto mejor resultado mejor nota)\n",
    "- Presentación final del informe, calidad de la redacción, uso de información obtenida en el TP1, conclusiones presentadas.\n",
    "- Performance de la solución final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías a usar\n",
    "# para escapear html\n",
    "!pip install bs4\n",
    "# para normalización del texto\n",
    "!pip install nltk\n",
    "!pip install stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from html import unescape\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "#Para generar una paleta de colores equivalente a cubehelix(sns) en matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('default') # haciendo los graficos un poco mas bonitos en matplotlib\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "\n",
    "sns.set(style=\"whitegrid\") # seteando tipo de grid en seaborn\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "#Nueva paleta de colores para matplotlib\n",
    "color_hex_arr = [\"#2B6F39\", \"#C2D8F2\",\"#182D48\",\"#A1784A\",\"#D38FC5\"]\n",
    "cubehelix_map = ListedColormap(sns.color_palette(color_hex_arr).as_hex())\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv', encoding='utf-8')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quitamos la columna Location\n",
    "Del análisis del tp1 vimos que la columna location tenía muchos valores nulos y muchísimos valores únicos, muchas veces con textos sin sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['location'], axis=1, inplace=True)\n",
    "train.drop(['location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reemplazamos los None de keyword por un texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el daset tweets para después poder aplicarle todas las tranformaciones juntas, después volvemos a dividir\n",
    "tweets = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregado de nuevos features y limpieza del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primera normalización del texto, pasamos a lowercase \n",
    "tweets['normalized_text'] = tweets.text.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización del texto y nuevas columnas a partir del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los links del texto normalizado y guardamos los links en una columna a parte por si sirven a futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLPATTERN = r'(https?://\\S+)' \n",
    "\n",
    "tweets['urls'] = tweets.normalized_text.apply(lambda x: re.findall(URLPATTERN, x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(URLPATTERN,\"\", x))\n",
    "# cuento la cantidad de links en los tweets\n",
    "tweets['url_count'] = tweets.urls.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los hashtags del texto normalizado y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de hashtags en los tweets\n",
    "# nueva columna con el total de hashtags, y los hashtags\n",
    "tweets['hashtags'] = tweets.normalized_text.apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "tweets['hashtags_count'] = tweets.hashtags.str.len()\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"#(\\w+)\",\"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos los tags del texto y lo dejamos en otra columna junto con el count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuento la cantidad de ags en los tweets\n",
    "# nueva columna con el total de tags, y los tags\n",
    "tweets['tags'] = tweets.text.str.lower().apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(lambda x: re.sub(r\"@(\\w+)\",\"\", x))\n",
    "tweets['tags_count'] = tweets.tags.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contamos cantidad de oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentences_count'] = tweets.normalized_text.apply(lambda x : len(sent_tokenize(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos signos de puntuación y html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) \n",
    "# !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuation(text):        \n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmlsymbols(text):\n",
    "    soup = BeautifulSoup(unescape(text))\n",
    "    return soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_htmlsymbols)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_punctuation)\n",
    "tweets['normalized_text'] = tweets.normalized_text.apply(remove_emojis_non_ascii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos stop words y creamos nueva columna con array de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el texto en listado de palabras y despues borramos las stop words\n",
    "tweets['words'] = tweets.normalized_text.str.split()\n",
    "stop_words = stopwords.words('english')\n",
    "# antes de eliminarlas las contamos\n",
    "tweets['stop_words_count'] = tweets['words'].apply(lambda x: len([item for item in x if item in stop_words]))\n",
    "tweets['normalized_words'] = tweets['words'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# eliminar las stop words del texto normalizado\n",
    "tweets['normalized_text'] = [' '.join(map(str, l)) for l in tweets['normalized_words']]\n",
    "# vemos como queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['words_count'] = tweets['text'].apply(lambda x: len(str(x).split()))\n",
    "tweets['unique_words_count'] = tweets['text'].apply(lambda x: len(set(str(x).split())))\n",
    "tweets['mean_word_length'] = tweets['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "tweets['text_len'] = tweets['text'].apply(lambda x: len(str(x)))\n",
    "tweets['punctuation_count'] = tweets['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contador de palabras y frecuencia de aparición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contamos la cantidad de palabras de cada tweet\n",
    "tweets['words_counter'] = tweets.normalized_words.apply(Counter)\n",
    "# me quedo con las palabras que mas ocurrencias tienen en cada row\n",
    "tweets['word_max_appearance'] = tweets.words_counter.apply( lambda x: max(x) if x else None)  \n",
    "tweets['word_max_appearance_count'] =  tweets.words_counter.apply( lambda x: max(x.values()) if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizamos si el texto tiene números y guardamos el dato en una nueva columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto chequea números como 1,2 y tambíen escrito como one, two\n",
    "def existence_of_numeric_data(text):\n",
    "    count = 0\n",
    "    text=nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    count = 0\n",
    "    for i in range(len(pos)):\n",
    "        word , pos_tag = pos[i]\n",
    "        if pos_tag == 'CD':\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con 1 si tiene , 0 sino\n",
    "tweets['numbers_count'] = tweets.normalized_text.apply(existence_of_numeric_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización de la columna keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos \"%20\" que representa espacio\n",
    "tweets['keyword'] = tweets.keyword.str.replace('%20',' ')\n",
    "train['keyword'] = train.keyword.str.replace('%20',' ')\n",
    "test['keyword'] = test.keyword.str.replace('%20',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correccion tipo de datos en keyword para optimizacion de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].astype('category')\n",
    "train['keyword'] = train['keyword'].astype('category')\n",
    "test['keyword'] = test['keyword'].astype('category')\n",
    "#check de tipos de datos categoricos \n",
    "tweets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregado de columnas para Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a float for sentiment strength based on the input text. Positive values are positive valence, negative value are negative valence.\n",
    "tweets['sentiment_score'] = tweets.normalized_text.apply(lambda x: sid.polarity_scores(x))\n",
    "tweets['sentiment_score_compound'] = tweets.sentiment_score.apply(lambda x: x['compound'])\n",
    "tweets['sentiment_score_pos'] = tweets.sentiment_score.apply(lambda x: x['pos'])\n",
    "tweets['sentiment_score_neg'] = tweets.sentiment_score.apply(lambda x: x['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vemos cómo queda el dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de datasets para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar submission file\n",
    "def generate_submission_csv(X,y,file):\n",
    "    result = pd.DataFrame()\n",
    "    result['id'] = X.index\n",
    "    result['target']=y\n",
    "    result.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, classification_report,confusion_matrix\n",
    "\n",
    "def print_scores(y_val, y_pred):\n",
    "    print (f'f1-macro { f1_score(y_val, y_pred, average=\"macro\") }' )\n",
    "    print (f'f1-micro {f1_score(y_val, y_pred, average=\"micro\")}')\n",
    "    print (f'f1-weighted {f1_score(y_val, y_pred, average=\"weighted\")}')\n",
    "    print (f'f1 {f1_score(y_val, y_pred, average=None)}')\n",
    "    print(f'accuracy {accuracy_score(y_val, y_pred)}')\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volvemos a separar los sets de datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos las columnas que quedaron de solo texto, salvo el texto normalizado\n",
    "tweets.drop(['text','keyword', 'urls', 'hashtags', 'tags', 'words', 'normalized_words', 'words_counter', 'word_max_appearance',\n",
    "       'sentiment_score',  'sentiment_score_pos','sentiment_score_neg', 'target'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperamos el dataset de test con los nuevos features\n",
    "test = pd.concat([tweets.set_index('id'),test.set_index('id')], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperamos el dataset de train con los nuevos features\n",
    "train = pd.concat([tweets.set_index('id'),train.set_index('id')], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entrenamiento: prueba 1 usando solo el texto para predecir y TF-IDF para transformar y MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature = train['normalized_text']\n",
    "# create the object of tfid vectorizer\n",
    "tfid_vectorizer = TfidfVectorizer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(text_feature, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación del set de datos train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer using the text data\n",
    "X_train_transformed = tfid_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del set de datos train con MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación del set de test y predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer using the text data\n",
    "X_val_transformed = tfid_vectorizer.transform(X_val)\n",
    "y_pred = clf.predict(X_val_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis con métrica f1 score y accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción sobre el set de Test que se sube a kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the testing set you only need to transform()\n",
    "X_test_1 = tfid_vectorizer.transform(test['normalized_text'])\n",
    "y_test_1 = clf.predict(X_test_1)\n",
    "y_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(test, y_test_1,'prueba_TF-IDF_MultinomialNB_text_normalizado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento: prueba 2 usando solo el texto para predecir y el keyword concatenado al texto y TF-IDF para transformar y MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = train\n",
    "# TODO revisar esto lo vuelvo a text para poder hacer la transformacion de nan a texto vacio\n",
    "train_2['keyword'] = train_2['keyword'].astype('object')\n",
    "train_2['keyword'] = train_2['keyword'].replace(np.nan, '', regex=True)\n",
    "train_2['keyword'] = train_2['keyword'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2.keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_feature_2 = train_2['normalized_text'].str.cat(train_2['keyword'], sep =\" \") \n",
    "text_feature_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train_2['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(text_feature_2, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación del set de datos train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer using the text data\n",
    "X_train_transformed = tfid_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del set de datos train con MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformación del set de test y predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the vectorizer using the text data\n",
    "X_val_transformed_2 = tfid_vectorizer.transform(X_val)\n",
    "y_pred_2 = clf.predict(X_val_transformed_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis con métrica f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_scores(y_val, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción sobre el set de Test que se sube a kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = test\n",
    "# TODO revisar esto lo vuelvo a text para poder hacer la transformacion de nan a texto vacio\n",
    "test_2['keyword'] = test_2['keyword'].astype('object')\n",
    "test_2['keyword'] = test_2['keyword'].replace(np.nan, '', regex=True)\n",
    "test_2['keyword'] = test_2['keyword'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the testing set you only need to transform()\n",
    "X_test_2 = tfid_vectorizer.transform(test_text_feature)\n",
    "y_test_2 = clf.predict(X_test_2)\n",
    "y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(test_2, y_test_2,'prueba_TF-IDF_MultinomialNB_text_normalizado_keyword.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_feature = test_2['normalized_text'].str.cat(test_2['keyword'], sep =\" \") \n",
    "test_text_feature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento: prueba 3 usando mas features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"keyword\"] = train[\"keyword\"].astype('object').fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textTransformer_1 = Pipeline(steps=[('text_bow1', CountVectorizer())])\n",
    "\n",
    "numeric_features = ['url_count', 'hashtags_count', 'tags_count', 'sentences_count', 'stop_words_count', 'words_count', 'unique_words_count', 'mean_word_length', 'text_len',\n",
    "       'punctuation_count', 'word_max_appearance_count', 'numbers_count', 'sentiment_score_compound']\n",
    "\n",
    "numTransformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numTransformer, numeric_features),\n",
    "        ('text1', textTransformer_1, 'keyword'),        \n",
    "        ('text_tfidf',  TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2)), 'normalized_text')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(text):    \n",
    "    '''a function which stems each word in the given text'''\n",
    "    text = [stemmer.stem(word) for word in text.split()]\n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "train_3 = train[['normalized_text', 'url_count', 'hashtags_count', 'tags_count','sentences_count', 'stop_words_count', 'words_count',\n",
    "       'unique_words_count', 'mean_word_length', 'text_len','punctuation_count', 'word_max_appearance_count', 'numbers_count',\n",
    "       'sentiment_score_compound', 'keyword']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_3['normalized_text'] = train_3['normalized_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_3, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "# pipeline model = Pipeline(steps=[('feature_engineer', preprocess),('RC', RidgeClassifier())])\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(\n",
    "    preprocess,\n",
    "    RidgeClassifier())\n",
    "\n",
    "# RandomForestClassifier(n_jobs=-1, class_weight='balanced')\n",
    "# MultinomialNB()\n",
    "clf = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3 = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con stemmer y RidgeClassifier\n",
    "print_scores(y_val, y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the testing set you only need to transform()\n",
    "test_3 = test\n",
    "test_3[\"keyword\"] = test_3[\"keyword\"].astype('object').fillna(\"unknown\")\n",
    "test_3['normalized_text'] = test_3['normalized_text'].apply(stemming)\n",
    "y_test_3 = clf.predict(test_3)\n",
    "y_test_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {},
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
