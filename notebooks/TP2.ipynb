{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Enunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo TP es una competencia de Machine Learning en donde cada grupo debe intentar determinar, para cada tweet brindado, si el mismo esta basado en un hecho real o no.\n",
    "\n",
    "La competencia se desarrolla en la plataforma de Kaggle  https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "El dataset consta de una serie de tweets, para los cuales se informa:\n",
    "\n",
    "<br/>\n",
    "\n",
    "* id - identificador unico para cada  tweet\n",
    "* text - el texto del tweet\n",
    "* location - ubicación desde donde fue enviado (podría no estar)\n",
    "* keyword - un keyword para el tweet  (podría faltar)\n",
    "* target - en train.csv, indica si se trata de un desastre real  (1) o no (0)\n",
    " \n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Los submits con el resultado deben tener el formato:\n",
    "\n",
    "Id: Un id numérico para identificar el tweet\n",
    "target: 1 / 0 según se crea que el tweet se trata sobre un desastre real, o no.\n",
    "\n",
    "Los grupos deberán probar distintos algoritmos de Machine Learning para intentar predecir si el tweet está basado en hechos reales o no. A medida que los grupos realicen pruebas deben realizar el correspondiente submit en Kaggle para evaluar el resultado de los mismos.\n",
    "\n",
    "Al finalizar la competencia el grupo que mejor resultado tenga obtendrá 10 puntos para cada uno de sus integrantes que podrán ser usados en el examen por promoción o segundo recuperatorio.\n",
    "\n",
    "Requisitos para la entrega del TP2:\n",
    "\n",
    "- El TP debe programarse en Python o R.\n",
    "- Debe entregarse un pdf con el informe de algoritmos probados, algoritmo final utilizado, transformaciones realizadas a los datos, feature engineering, etc. \n",
    "- El informe debe incluir también un link a github con el informe presentado en pdf, y todo el código.\n",
    "- El grupo debe presentar el TP en una computadora en la fecha indicada por la cátedra, el TP debe correr en un lapso de tiempo razonable (inferior a 1 hora) y generar un submission válido que iguale el mejor resultado obtenido por el grupo en Kaggle. (mas detalles a definir)\n",
    "\n",
    "El TP2 se va a evaluar en función del siguiente criterio:\n",
    "\n",
    "- Cantidad de trabajo (esfuerzo) del grupo: ¿Probaron muchos algoritmos? ¿Hicieron un buen trabajo de pre-procesamiento de los datos y feature engineering?\n",
    "- Resultado obtenido en Kaggle (obviamente cuanto mejor resultado mejor nota)\n",
    "- Presentación final del informe, calidad de la redacción, uso de información obtenida en el TP1, conclusiones presentadas.\n",
    "- Performance de la solución final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesado\n",
    "#### Introducción \n",
    "Se levantan los datos como en el TP1. Sin EDA, solo el preprocesado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Instalación de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install stopwords\n",
    "\n",
    "!pip install sklearn\n",
    "!pip install xgboost==0.7.post4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron\n",
    "from sklearn.metrics import f1_score,roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from time import process_time\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Obtención de datos\n",
    "Lectura de datos de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "tweets_test = pd.read_csv('../data/test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limpieza de datos.\n",
    "#### Introducción\n",
    "Antes de empezar, hay que normalizar el texto ya que luego de la tokenización serán convertidos en vectores dentro de una matriz, las técnicas a utilizar:\n",
    "* **Uppercase/lowercase**: Paso todo a lower/upper case, ya que una misma palabra tiene una representación distinta si se hay un cambio de mayúscula minúscula.\n",
    "* **Limpieza de texto**: Signos de puntuación, valores numéricos, links, carácteres especiales, etc.\n",
    "* **Tokenizacion**: Es el proceso de convertir el texto en una lista de tokens,\n",
    "* **Stopwords**: Elimino palabras comunes que no aportan información\n",
    "* **Stemming**: Elimino los sufijos de palabras que puedan tener el mismo significado (o función dentro del texto)\n",
    "* **Lemmatization**: Unifico palabras que signifiquen lo mismo en base a su definición del diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializo dataset para probar las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copia de datasets para trabajar el pre-procesado de texto\n",
    "train_df1 = tweets_train.copy()\n",
    "test_df1  = tweets_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Uppercase + Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar emojis, viene del tp1\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result\n",
    "\n",
    "\n",
    "#Funcion para limpieza del texto (todo a LOWERCASE)\n",
    "def text_clean(text):\n",
    "    text = text.lower()\n",
    "    text = remove_emojis_non_ascii(text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplico la funcion a la copia de los Dataset de entrenamiento y test\n",
    "train_df1['text'] = train_df1['text'].apply(lambda x: text_clean(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Tokenización\n",
    "_Probar los distintos que ofrece la librería nltk_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para tokenizar utilizo el RegEx tokenizer de nltk\n",
    "#tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Para tokenizar utilizo WhitespaceTokenizer\n",
    "#tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "#Para tokenizar utilizo WordPunctTokenizer\n",
    "#tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "#Para tokenizar utilizo TreebankWordTokenizer\n",
    "self_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x: self_tokenizer.tokenize(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: self_tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar Stopwords\n",
    "def text_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x : text_stopwords(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 Stemming + Lemmatizing\n",
    "Probar si aportan algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para Stemming y Lemmatizing\n",
    "def text_stemming(text):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    text_stemmed = \" \".join(stemmer.stem(token) for token in tokens)\n",
    "    return text_stemmed\n",
    "\n",
    "def text_lemmatizing(text):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    text_lemmatized = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combino el texto para luego de haberlo procesado\n",
    "def text_combine(text):\n",
    "    comb_text = ' '.join(text)\n",
    "    return comb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x : text_combine(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_combine(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1.head() ##Datos Antes del lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1.head() ##Datos luego del lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 Pre-procesado de texto\n",
    "Devuelve texto, agregar una para devolver tambien solo TOKENS, ya que es lo que se va a utilizar para entrenar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text): \n",
    "    cleaned_txt = text_clean(text)\n",
    "    lemma_text = text_lemmatizing(cleaned_txt)\n",
    "    tokenized_text = self_tokenizer.tokenize(lemma_text)    \n",
    "    remove_stopwords = text_stopwords(tokenized_text)\n",
    "    combined_text = text_combine(remove_stopwords)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df1['text'] = test_df1['text'].apply(lambda x : pre_process_text(x))\n",
    "test_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorización del texto\n",
    "Para entrenar el modelo necesitamos convertir el texto a una matriz de vectores para que pueda interpretarlo, \n",
    "para lograrlo existen distintas técnicas.\n",
    "\n",
    "\n",
    "* Bag of Words\n",
    "* TF-IDF\n",
    "* N-Gramas\n",
    "* Feature Hashing\n",
    "* Red convolucional 1-D\n",
    "\n",
    "\n",
    "Cada una de estas alternativas esta directamente relacionada con la transformación del texto (Tokenizacion, limpieza, lemming, stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Preparacion de datasets\n",
    "Preparo datasets de train y test aplicando el preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2=tweets_train.copy()\n",
    "train_df2['text'] = train_df2['text'].apply(lambda x : pre_process_text(x))\n",
    "\n",
    "test_df2=tweets_test.copy()\n",
    "test_df2['text'] = test_df2['text'].apply(lambda x : pre_process_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of Words\n",
    "Se crea un diccionario de palabras conocidas, luego de eso se representa el texto en un vector donde cada posición indica la existencia (o no) de las palabras.\n",
    "\n",
    "#### CountVectorize\n",
    "\n",
    "CountVectorize convierte una coleccion de documentos a una matriz de tokens contabilizados. Esta funcion incluye varios metodos para preprocedo/tokenizacion/stopwords, por lo que se podría modificar desde la siguiente línea. Sin embargo, como ya se hizo el pre-procesado del texto solo voy a usar la función sin ningun feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizacion con countVectorize\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_cv = count_vectorizer.fit_transform(train_df2['text'])\n",
    "test_cv = count_vectorizer.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF\n",
    "Tf-idf (Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en la colección de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección. \n",
    "Es una mejora de Bag of Words ya que contabiliza y pondera las palabras en base a su frecuencia de aparición en el documento, por ejemplo la palabra \"the\" puede tener muchas apariciones en el texto, por lo que se podria dar una importancia menor.\n",
    "\n",
    "#### **Calculo TD-IDF**\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Term Frequency(TF)**: Es la ponderación de la palabra dentro del documento\n",
    "\n",
    "$ {\\displaystyle tf} = \\frac{fdt}{nT}$\n",
    "<br/>\n",
    "Donde:\n",
    "* $ fdt $: Frecuencia de aparición del término t en el documento\n",
    "* $ nT $: Número de términos en el documento\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Inverse Document Frequency(IDF)**: Es el valor de que tan \"rara\" es la palabra a través de todos los documentos\n",
    "\n",
    "$ {\\displaystyle idf} = 1+\\log(\\frac{N}{n}) $ \n",
    "<br/>\n",
    "Donde:\n",
    "* $ N $: numero de documentos\n",
    "* $ n $: numero de documentos con aparición del termino t\n",
    "\n",
    "<br/>\n",
    "\n",
    "**TF-IDF**: La ponderación del termino por tf-idf está dada por\n",
    "\n",
    "$ {\\displaystyle tfidf}(w,d,D) = {\\displaystyle tf}(w,d) \\times {\\displaystyle idf}(w,D) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizacion utilizando TF-IDF (UNI Y BI-GRAMAS)\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tf = tfidf.fit_transform(train_df2['text'])\n",
    "test_tf = tfidf.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 N-Gramas\n",
    "Agrupo las palabras en grupos de 1,2,3,n palabras, para agregarles un contexto.\n",
    "\n",
    "Esto se puede lograr utilizando countVectorize para analizar la frecuencia de aparición de n-gramas o combinarlo con tf-idf para considerar la ponderación del término en base a sus apariciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupo por bi-gramas y tri-gramas con CountVectorizer\n",
    "ngram_cv = CountVectorizer(ngram_range=(2,3))\n",
    "train_ng_cv = ngram_cv.fit_transform(train_df2['text'])\n",
    "test_ng_cv = ngram_cv.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupo por bi-gramas y tri-gramas con TF-IDF\n",
    "ngram_tf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(2, 3))\n",
    "train_ng_tf = ngram_tf.fit_transform(train_df2['text'])\n",
    "test_ng_tf = ngram_tf.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Hashing\n",
    "Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Red Convolucional de 1 dimension\n",
    "Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento del modelo \n",
    "Para el entrenamiento pruebo algunos algoritmos _(en verde los probados, en rojo los descartados por ineficientes)_\n",
    "* <font color='green'>Logistic Regression </font>\n",
    "* <font color='green'>Decision tree</font>\n",
    "* <font color='green'>KNN</font>\n",
    "* <font color='green'>Gradient Boosting Clasifier</font>\n",
    "* <font color='green'>Random Forest</font>\n",
    "* <font color='green'>RidgeClassifier</font>\n",
    "* <font color='green'>MNB (MultinomialNB)</font>\n",
    "* <font color='green'>Perceptron</font>\n",
    "* <font color='green'>xgBoost</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Organizo algoritmos\n",
    "Para tener un poco mas ordenado todo, agrupo los algortimos en una colección para luego poder evaluarlos en bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo diccionario con los modelos de regresion a probar.\n",
    "modelsDict = {    \n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),  \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=15),\n",
    "    'MNB': MultinomialNB(),\n",
    "    'RidgeClassifier': RidgeClassifier(class_weight='balanced'),\n",
    "    'Perceptron': Perceptron(class_weight='balanced'),\n",
    "    'xgboost': XGBClassifier(n_estimators=10),\n",
    "    \"Logistic Regression\": LogisticRegression(C=1.0)\n",
    "    }\n",
    "\n",
    "no_classifiers = len(modelsDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_classify(x_train, y_train, x_test, y_test):\n",
    "    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,6)), columns = ['Clasificador', 'Prec. train', 'Prec. test','AUC score','F1', 'Tiempo transcurrido'])\n",
    "    count = 0\n",
    "    for key, classifier in modelsDict.items():\n",
    "        \n",
    "        t_start = process_time()  \n",
    "        classifier.fit(x_train, y_train)\n",
    "        t_stop = process_time() \n",
    "        t_elapsed = t_stop - t_start\n",
    "        y_predicted = classifier.predict(x_test)\n",
    "        \n",
    "        df_results.loc[count,'Clasificador'] = key\n",
    "        df_results.loc[count,'AUC score'] = roc_auc_score(y_test, y_predicted)\n",
    "        df_results.loc[count,'Prec. train'] = round(classifier.score(x_train, y_train)*100)\n",
    "        df_results.loc[count,'Prec. test'] =round(accuracy_score(y_test,y_predicted)*100) \n",
    "        df_results.loc[count,'F1'] = f1_score(y_test, y_predicted, zero_division=1)\n",
    "        df_results.loc[count,'Tiempo transcurrido'] = t_elapsed    \n",
    "              \n",
    "        count+=1\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Datos para countVector\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "cv_results = batch_classify(x_train_cv, y_train_cv,x_test_cv, y_test_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para TF-IDF\n",
    "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(train_tf,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "tf_results = batch_classify(x_train_tf, y_train_tf,x_test_tf, y_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para countVector + n-gramas\n",
    "x_train_ng_cv, x_test_ng_cv, y_train_ng_cv, y_test_ng_cv =train_test_split(train_ng_cv,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "cv_ng_results = batch_classify(x_train_ng_cv, y_train_ng_cv,x_test_ng_cv, y_test_ng_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para TF-IDF + n-gramas\n",
    "x_train_ng_tf, x_test_ng_tf, y_train_ng_tf, y_test_ng_tf = train_test_split(train_ng_tf,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "tf_ng_results = batch_classify(x_train_ng_tf, y_train_ng_tf,x_test_ng_tf, y_test_ng_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resultados\n",
    "Comparo la performance de los distintos modelos probados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo resultados para countVector\n",
    "cv_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo resultados para TF-IDF\n",
    "tf_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo resultados para countVector + n-gramas\n",
    "cv_ng_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimo resultados para TF-IDF + n-gramas\n",
    "tf_ng_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Envío de datos\n",
    "Preparo el submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(model,test_vector):\n",
    "    \n",
    "    '''Input- model=final fit model to be used for predictions\n",
    "              test_vector=pre-processed and vectorized test dataset\n",
    "       Output- submission file in .csv format with predictions       \n",
    "    \n",
    "    '''    \n",
    "    sub_df = pd.read_csv('../data/sample_submission.csv')\n",
    "    sub_df[\"target\"] = model.predict(test_vector)\n",
    "    sub_df.to_csv(\"submission.csv\", index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB + countVector\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv, y_train_cv)\n",
    "submission(mnb_model,test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression +TF-IDF (1-2-gramas)\n",
    "lr_model = LogisticRegression(C=1.0)\n",
    "lr_model.fit(x_train_tf, y_train_tf)\n",
    "submission(lr_model,test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB + TF-IDF (1-2-gramas)\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_tf, y_train_tf)\n",
    "submission(mnb_model,test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
