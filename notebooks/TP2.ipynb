{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Enunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo TP es una competencia de Machine Learning en donde cada grupo debe intentar determinar, para cada tweet brindado, si el mismo esta basado en un hecho real o no.\n",
    "\n",
    "La competencia se desarrolla en la plataforma de Kaggle  https://www.kaggle.com/c/nlp-getting-started.  \n",
    "\n",
    "El dataset consta de una serie de tweets, para los cuales se informa:\n",
    "\n",
    "<br/>\n",
    "\n",
    "* id - identificador unico para cada  tweet\n",
    "* text - el texto del tweet\n",
    "* location - ubicación desde donde fue enviado (podría no estar)\n",
    "* keyword - un keyword para el tweet  (podría faltar)\n",
    "* target - en train.csv, indica si se trata de un desastre real  (1) o no (0)\n",
    " \n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Los submits con el resultado deben tener el formato:\n",
    "\n",
    "Id: Un id numérico para identificar el tweet\n",
    "target: 1 / 0 según se crea que el tweet se trata sobre un desastre real, o no.\n",
    "\n",
    "Los grupos deberán probar distintos algoritmos de Machine Learning para intentar predecir si el tweet está basado en hechos reales o no. A medida que los grupos realicen pruebas deben realizar el correspondiente submit en Kaggle para evaluar el resultado de los mismos.\n",
    "\n",
    "Al finalizar la competencia el grupo que mejor resultado tenga obtendrá 10 puntos para cada uno de sus integrantes que podrán ser usados en el examen por promoción o segundo recuperatorio.\n",
    "\n",
    "Requisitos para la entrega del TP2:\n",
    "\n",
    "- El TP debe programarse en Python o R.\n",
    "- Debe entregarse un pdf con el informe de algoritmos probados, algoritmo final utilizado, transformaciones realizadas a los datos, feature engineering, etc. \n",
    "- El informe debe incluir también un link a github con el informe presentado en pdf, y todo el código.\n",
    "- El grupo debe presentar el TP en una computadora en la fecha indicada por la cátedra, el TP debe correr en un lapso de tiempo razonable (inferior a 1 hora) y generar un submission válido que iguale el mejor resultado obtenido por el grupo en Kaggle. (mas detalles a definir)\n",
    "\n",
    "El TP2 se va a evaluar en función del siguiente criterio:\n",
    "\n",
    "- Cantidad de trabajo (esfuerzo) del grupo: ¿Probaron muchos algoritmos? ¿Hicieron un buen trabajo de pre-procesamiento de los datos y feature engineering?\n",
    "- Resultado obtenido en Kaggle (obviamente cuanto mejor resultado mejor nota)\n",
    "- Presentación final del informe, calidad de la redacción, uso de información obtenida en el TP1, conclusiones presentadas.\n",
    "- Performance de la solución final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocesado\n",
    "#### Introducción \n",
    "Se levantan los datos como en el TP1. Sin EDA, solo el preprocesado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Instalación de librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/lib/python3/dist-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.46.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2020.5.7)\n",
      "Requirement already satisfied: stopwords in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from wordcloud) (1.18.4)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from wordcloud) (3.2.1)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from wordcloud) (6.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from matplotlib->wordcloud) (2.7.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.18.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from plotly) (1.12.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.18.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.18.4)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.14)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.0.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.12.0)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.8.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/lib/python3/dist-packages (from pandas>=0.24.0->catboost) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/lib/python3/dist-packages (from pandas>=0.24.0->catboost) (2.7.3)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.2.0)\n",
      "Requirement already satisfied: xgboost==0.7.post4 in /usr/local/lib/python3.7/dist-packages (0.7.post4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost==0.7.post4) (1.18.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost==0.7.post4) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías a usar\n",
    "# para escapear html\n",
    "!pip3 install bs4\n",
    "# para normalización del texto\n",
    "!pip3 install nltk\n",
    "!pip3 install stopwords\n",
    "# visualización nube de palabras\n",
    "!pip3 install wordcloud\n",
    "!pip3 install matplotlib\n",
    "!pip3 install plotly\n",
    "!pip3 install sklearn\n",
    "!pip3 install catboost\n",
    "!pip3 install xgboost==0.7.post4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import re\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from platform import python_version\n",
    "from time import process_time \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Obtención de datos\n",
    "Lectura de datos de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = pd.read_csv('../data/train.csv', encoding='utf-8')\n",
    "tweets_test = pd.read_csv('../data/test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Limpieza de datos.\n",
    "#### Introducción\n",
    "Antes de empezar, hay que normalizar el texto ya que luego de la tokenización serán convertidos en vectores dentro de una matriz, las técnicas a utilizar:\n",
    "* **Uppercase/lowercase**: Paso todo a lower/upper case, ya que una misma palabra tiene una representación distinta si se hay un cambio de mayúscula minúscula.\n",
    "* **Limpieza de texto**: Signos de puntuación, valores numéricos, links, carácteres especiales, etc.\n",
    "* **Tokenizacion**: Es el proceso de convertir el texto en una lista de tokens,\n",
    "* **Stopwords**: Elimino palabras comunes que no aportan información\n",
    "* **Stemming**: Elimino los sufijos de palabras que puedan tener el mismo significado (o función dentro del texto)\n",
    "* **Lemmatization**: Unifico palabras que signifiquen lo mismo en base a su definición del diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializo dataset para probar las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copia de datasets para trabajar el pre-procesado de texto\n",
    "train_df1 = tweets_train.copy()\n",
    "test_df1  = tweets_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Uppercase + Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar emojis, viene del tp1\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def remove_emojis_non_ascii(text):    \n",
    "    #replace consecutive non-ASCII characters with a space\n",
    "    result = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    #remove emojis from tweet\n",
    "    result = emoji_pattern.sub(r'', result)    \n",
    "    return result\n",
    "\n",
    "\n",
    "#Funcion para limpieza del texto (todo a LOWERCASE)\n",
    "def text_clean(text):\n",
    "    text = text.lower()\n",
    "    text = remove_emojis_non_ascii(text)\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplico la funcion a la copia de los Dataset de entrenamiento y test\n",
    "train_df1['text'] = train_df1['text'].apply(lambda x: text_clean(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: text_clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Tokenización\n",
    "_Probar los distintos que ofrece la librería nltk_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para tokenizar utilizo el RegEx tokenizer de nltk\n",
    "#tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#Para tokenizar utilizo WhitespaceTokenizer\n",
    "#tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "#Para tokenizar utilizo WordPunctTokenizer\n",
    "#tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "#Para tokenizar utilizo TreebankWordTokenizer\n",
    "self_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x: self_tokenizer.tokenize(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x: self_tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para eliminar Stopwords\n",
    "def text_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x : text_stopwords(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.4 Stemming + Lemmatizing\n",
    "Probar si aportan algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para Stemming y Lemmatizing\n",
    "def text_stemming(text):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    text_stemmed = \" \".join(stemmer.stem(token) for token in tokens)\n",
    "\n",
    "def text_lemmatizing(text):\n",
    "    tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "    text_lemmatized = \" \".join(lemmatizer.lemmatize(token) for token in tokens)\n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combino el texto para luego de haberlo procesado\n",
    "def text_combine(text):\n",
    "    comb_text = ' '.join(text)\n",
    "    return comb_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df1['text'] = train_df1['text'].apply(lambda x : text_combine(x))\n",
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_combine(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                        happened terrible car crash\n",
       "1   2     NaN      NaN  heard earthquake different cities stay safe ev...\n",
       "2   3     NaN      NaN  forest fire spot pond geese fleeing across str...\n",
       "3   9     NaN      NaN              apocalypse lighting spokane wildfires\n",
       "4  11     NaN      NaN                typhoon soudelor kills china taiwan"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1.head() ##Datos Antes del lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1['text'] = test_df1['text'].apply(lambda x : text_lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                        happened terrible car crash\n",
       "1   2     NaN      NaN  heard earthquake different city stay safe ever...\n",
       "2   3     NaN      NaN  forest fire spot pond goose fleeing across str...\n",
       "3   9     NaN      NaN               apocalypse lighting spokane wildfire\n",
       "4  11     NaN      NaN                 typhoon soudelor kill china taiwan"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1.head() ##Datos luego del lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.5 Pre-procesado de texto\n",
    "Devuelve texto, agregar una para devolver tambien solo TOKENS, ya que es lo que se va a utilizar para entrenar al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_text(text): \n",
    "    cleaned_txt = text_clean(text)\n",
    "    lemma_text = text_lemmatizing(cleaned_txt)\n",
    "    tokenized_text = self_tokenizer.tokenize(lemma_text)    \n",
    "    remove_stopwords = text_stopwords(tokenized_text)\n",
    "    combined_text = text_combine(remove_stopwords)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                        happened terrible car crash\n",
       "1   2     NaN      NaN  heard earthquake different city stay safe ever...\n",
       "2   3     NaN      NaN  forest fire spot pond goose fleeing across str...\n",
       "3   9     NaN      NaN               apocalypse lighting spokane wildfire\n",
       "4  11     NaN      NaN                 typhoon soudelor kill china taiwan"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1['text'] = test_df1['text'].apply(lambda x : pre_process_text(x))\n",
    "test_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparo datasets para parte 2\n",
    "\n",
    "train_df2=tweets_train.copy()\n",
    "train_df2['text'] = train_df2['text'].apply(lambda x : pre_process_text(x))\n",
    "\n",
    "test_df2=tweets_test.copy()\n",
    "test_df2['text'] = test_df2['text'].apply(lambda x : pre_process_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorización del texto\n",
    "Para entrenar el modelo necesitamos convertir el texto a una matriz de vectores para que pueda interpretarlo, \n",
    "para lograrlo existen distintas técnicas.\n",
    "\n",
    "\n",
    "* Bag of Words\n",
    "* TF-IDF\n",
    "* N-Gramas\n",
    "* Feature Hashing\n",
    "* Red convolucional 1-D\n",
    "\n",
    "\n",
    "Cada una de estas alternativas esta directamente relacionada con la transformación del texto (Tokenizacion, limpieza, lemming, stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of Words\n",
    "Se crea un diccionario de palabras conocidas, luego de eso se representa el texto en un vector donde cada posición indica la existencia (o no) de las palabras.\n",
    "\n",
    "#### CountVectorize\n",
    "\n",
    "CountVectorize convierte una coleccion de documentos a una matriz de tokens contabilizados. Esta funcion incluye varios metodos para preprocedo/tokenizacion/stopwords, por lo que se podría modificar desde la siguiente línea. Sin embargo, como ya se hizo el pre-procesado del texto solo voy a usar la función sin ningun feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizacion con countVectorize\n",
    "count_vectorizer = CountVectorizer()\n",
    "train_cv = count_vectorizer.fit_transform(train_df2['text'])\n",
    "test_cv = count_vectorizer.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF\n",
    "Tf-idf (Term frequency – Inverse document frequency), frecuencia de término – frecuencia inversa de documento (o sea, la frecuencia de ocurrencia del término en la colección de documentos), es una medida numérica que expresa cuán relevante es una palabra para un documento en una colección. \n",
    "Es una mejora de Bag of Words ya que contabiliza y pondera las palabras en base a su frecuencia de aparición en el documento, por ejemplo la palabra \"the\" puede tener muchas apariciones en el texto, por lo que se podria dar una importancia menor.\n",
    "\n",
    "#### **Calculo TD-IDF**\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Term Frequency(TF)**: Es la ponderación de la palabra dentro del documento\n",
    "\n",
    "$ {\\displaystyle tf} = \\frac{fdt}{nT}$\n",
    "<br/>\n",
    "Donde:\n",
    "* $ fdt $: Frecuencia de aparición del término t en el documento\n",
    "* $ nT $: Número de términos en el documento\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Inverse Document Frequency(IDF)**: Es el valor de que tan \"rara\" es la palabra a través de todos los documentos\n",
    "\n",
    "$ {\\displaystyle idf} = 1+\\log(\\frac{N}{n}) $ \n",
    "<br/>\n",
    "Donde:\n",
    "* $ N $: numero de documentos\n",
    "* $ n $: numero de documentos con aparición del termino t\n",
    "\n",
    "<br/>\n",
    "\n",
    "**TF-IDF**: La ponderación del termino por tf-idf está dada por\n",
    "\n",
    "$ {\\displaystyle tfidf}(w,d,D) = {\\displaystyle tf}(w,d) \\times {\\displaystyle idf}(w,D) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizacion utilizando TF-IDF (UNI Y BI-GRAMAS)\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "train_tf = tfidf.fit_transform(train_df2['text'])\n",
    "test_tf = tfidf.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 N-Gramas\n",
    "Agrupo las palabras en grupos de 1,2,3,n palabras, para agregarles un contexto.\n",
    "\n",
    "Esto se puede lograr utilizando countVectorize para analizar la frecuencia de aparición de n-gramas o combinarlo con tf-idf para considerar la ponderación del término en base a sus apariciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupo por bi-gramas y tri-gramas con CountVectorizer\n",
    "ngram_cv = CountVectorizer(ngram_range=(2,3))\n",
    "train_ng_cv = ngram_cv.fit_transform(train_df2['text'])\n",
    "test_ng_cv = ngram_cv.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupo por bi-gramas y tri-gramas con TF-IDF\n",
    "ngram_tf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(2, 3))\n",
    "train_ng_tf = ngram_tf.fit_transform(train_df2['text'])\n",
    "test_ng_tf = ngram_tf.transform(test_df2[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Hashing\n",
    "Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Red Convolucional de 1 dimension\n",
    "Pendiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento del modelo \n",
    "Para el entrenamiento pruebo algunos algoritmos _(en verde los probados, en rojo los descartados por ineficientes)_\n",
    "* <font color='green'>Logistic Regression </font>\n",
    "* <font color='red'>SVC</font>\n",
    "* <font color='green'>Decision tree</font>\n",
    "* <font color='green'>KNN</font>\n",
    "* <font color='red'>Linear SVM</font>\n",
    "* <font color='red'>Gradient Boosting Clasifier</font>\n",
    "* <font color='red'>Random Forest</font>\n",
    "* RidgeClassifier\n",
    "* <font color='red'>adaBoost</font>\n",
    "* <font color='green'>MNB (MultinomialNB)</font>\n",
    "* <font color='green'>Perceptron</font>\n",
    "* <font color='green'>xgBoost</font>\n",
    "* <font color='red'>catBoost</font>\n",
    "* SoftMax\n",
    "\n",
    "***LIMPIAR*** una tablita porque sí ***LIMPIAR***\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>algoritmo/vectorizacion</th>\n",
    "        <th>BOW</th>\n",
    "        <th>TF-IDF</th>\n",
    "        <th>N-Grama</th>\n",
    "        <th>Feature Hashing</th>\n",
    "        <th>Conv 1-D</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Logistic Regression</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>SVC</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Decision tree</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>KNN</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Linear SVM</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Gradient Boosting Clasifier</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Random Forest</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>RidgeClassifier</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>adaBoost</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>MNB</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Perceptron</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>xgBoost</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>catBoost</td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Organizo algoritmos\n",
    "Para tener un poco mas ordenado todo, agrupo los algortimos en una colección para luego poder evaluarlos en bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo diccionario con los modelos de regresion a probar.\n",
    "modelsDict = {    \n",
    "    #Descartados\n",
    "    \n",
    "    #\"SVC\": SVC(),    \n",
    "    #\"Linear SVM\": SVC(class_weight='balanced'),\n",
    "    #\"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    #\"Random Forest\": RandomForestClassifier(),    \n",
    "    #'AdaBoost': AdaBoostClassifier(n_estimators=100),\n",
    "    #'catboost': CatBoostClassifier(verbose=0)\n",
    "    \n",
    "    #A prueba\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=15),\n",
    "    'MNB': MultinomialNB(),\n",
    "    'RidgeClassifier': RidgeClassifier(class_weight='balanced'),\n",
    "    'Perceptron': Perceptron(class_weight='balanced'),\n",
    "    'xgboost': XGBClassifier(n_estimators=50),\n",
    "    \"Logistic Regression\": LogisticRegression(C=1.0)\n",
    "    }\n",
    "\n",
    "no_classifiers = len(modelsDict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def batch_classify(x_train, y_train, x_test, y_test):\n",
    "    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,6)), columns = ['Clasificador', 'Prec. train', 'Prec. test','AUC score','F1', 'Tiempo transcurrido'])\n",
    "    count = 0\n",
    "    for key, classifier in modelsDict.items():\n",
    "        \n",
    "        t_start = process_time()  \n",
    "        classifier.fit(x_train, y_train)\n",
    "        t_stop = process_time() \n",
    "        t_elapsed = t_stop - t_start\n",
    "        y_predicted = classifier.predict(x_test)\n",
    "        \n",
    "        df_results.loc[count,'Clasificador'] = key\n",
    "        df_results.loc[count,'AUC score'] = roc_auc_score(y_test, y_predicted)\n",
    "        df_results.loc[count,'Prec. train'] = round(classifier.score(x_train, y_train)*100)\n",
    "        df_results.loc[count,'Prec. test'] =round(accuracy_score(y_test,y_predicted)*100) \n",
    "        df_results.loc[count,'F1'] = f1_score(y_test, y_predicted, zero_division=1)\n",
    "        df_results.loc[count,'Tiempo transcurrido'] = t_elapsed    \n",
    "              \n",
    "        count+=1\n",
    "\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Datos para countVector\n",
    "x_train_cv, x_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "cv_results = batch_classify(x_train_cv, y_train_cv,x_test_cv, y_test_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para TF-IDF\n",
    "x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(train_tf,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "tf_results = batch_classify(x_train_tf, y_train_tf,x_test_tf, y_test_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para countVector + n-gramas\n",
    "x_train_ng_cv, x_test_ng_cv, y_train_ng_cv, y_test_ng_cv =train_test_split(train_ng_cv,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "cv_ng_results = batch_classify(x_train_ng_cv, y_train_ng_cv,x_test_ng_cv, y_test_ng_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datos para TF-IDF + n-gramas\n",
    "x_train_ng_tf, x_test_ng_tf, y_train_ng_tf, y_test_ng_tf = train_test_split(train_ng_tf,tweets_train.target,test_size=0.2,random_state=2020)\n",
    "tf_ng_results = batch_classify(x_train_ng_tf, y_train_ng_tf,x_test_ng_tf, y_test_ng_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Resultados\n",
    "Comparo la performance de los distintos modelos probados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clasificador</th>\n",
       "      <th>Prec. train</th>\n",
       "      <th>Prec. test</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Tiempo transcurrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNB</td>\n",
       "      <td>91.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.792898</td>\n",
       "      <td>0.765828</td>\n",
       "      <td>0.002020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>96.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.789642</td>\n",
       "      <td>0.756195</td>\n",
       "      <td>0.374937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>98.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.762160</td>\n",
       "      <td>0.731411</td>\n",
       "      <td>0.150857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>99.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.757879</td>\n",
       "      <td>0.719493</td>\n",
       "      <td>1.457674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>98.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.732669</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.099615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.659502</td>\n",
       "      <td>0.509474</td>\n",
       "      <td>0.590347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k-Nearest Neighbors</td>\n",
       "      <td>66.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.585793</td>\n",
       "      <td>0.303483</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Clasificador  Prec. train  Prec. test  AUC score        F1  \\\n",
       "2                  MNB         91.0        80.0   0.792898  0.765828   \n",
       "6  Logistic Regression         96.0        80.0   0.789642  0.756195   \n",
       "3      RidgeClassifier         98.0        77.0   0.762160  0.731411   \n",
       "0        Decision Tree         99.0        77.0   0.757879  0.719493   \n",
       "4           Perceptron         98.0        74.0   0.732669  0.702222   \n",
       "5              xgboost         72.0        69.0   0.659502  0.509474   \n",
       "1  k-Nearest Neighbors         66.0        63.0   0.585793  0.303483   \n",
       "\n",
       "   Tiempo transcurrido  \n",
       "2             0.002020  \n",
       "6             0.374937  \n",
       "3             0.150857  \n",
       "0             1.457674  \n",
       "4             0.099615  \n",
       "5             0.590347  \n",
       "1             0.000508  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimo resultados para countVector\n",
    "cv_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clasificador</th>\n",
       "      <th>Prec. train</th>\n",
       "      <th>Prec. test</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Tiempo transcurrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>88.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.786102</td>\n",
       "      <td>0.745819</td>\n",
       "      <td>0.250353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNB</td>\n",
       "      <td>87.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.782523</td>\n",
       "      <td>0.739865</td>\n",
       "      <td>0.001794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>94.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.779268</td>\n",
       "      <td>0.754967</td>\n",
       "      <td>0.061819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>98.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.740692</td>\n",
       "      <td>0.705432</td>\n",
       "      <td>1.958429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>97.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.736072</td>\n",
       "      <td>0.706144</td>\n",
       "      <td>0.041663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.658324</td>\n",
       "      <td>0.508403</td>\n",
       "      <td>0.657185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k-Nearest Neighbors</td>\n",
       "      <td>63.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.565894</td>\n",
       "      <td>0.238961</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Clasificador  Prec. train  Prec. test  AUC score        F1  \\\n",
       "6  Logistic Regression         88.0        80.0   0.786102  0.745819   \n",
       "2                  MNB         87.0        80.0   0.782523  0.739865   \n",
       "3      RidgeClassifier         94.0        78.0   0.779268  0.754967   \n",
       "0        Decision Tree         98.0        75.0   0.740692  0.705432   \n",
       "4           Perceptron         97.0        74.0   0.736072  0.706144   \n",
       "5              xgboost         72.0        69.0   0.658324  0.508403   \n",
       "1  k-Nearest Neighbors         63.0        62.0   0.565894  0.238961   \n",
       "\n",
       "   Tiempo transcurrido  \n",
       "6             0.250353  \n",
       "2             0.001794  \n",
       "3             0.061819  \n",
       "0             1.958429  \n",
       "4             0.041663  \n",
       "5             0.657185  \n",
       "1             0.000552  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimo resultados para TF-IDF\n",
    "tf_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clasificador</th>\n",
       "      <th>Prec. train</th>\n",
       "      <th>Prec. test</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Tiempo transcurrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>98.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.713431</td>\n",
       "      <td>0.648973</td>\n",
       "      <td>0.097917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>99.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.703469</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>0.152579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>98.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.694193</td>\n",
       "      <td>0.576065</td>\n",
       "      <td>0.921008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>99.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.696356</td>\n",
       "      <td>0.590420</td>\n",
       "      <td>24.023519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNB</td>\n",
       "      <td>98.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.640798</td>\n",
       "      <td>0.663986</td>\n",
       "      <td>0.005670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.574360</td>\n",
       "      <td>0.267516</td>\n",
       "      <td>2.250857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k-Nearest Neighbors</td>\n",
       "      <td>62.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.555791</td>\n",
       "      <td>0.202397</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Clasificador  Prec. train  Prec. test  AUC score        F1  \\\n",
       "4           Perceptron         98.0        73.0   0.713431  0.648973   \n",
       "3      RidgeClassifier         99.0        73.0   0.703469  0.602527   \n",
       "6  Logistic Regression         98.0        73.0   0.694193  0.576065   \n",
       "0        Decision Tree         99.0        72.0   0.696356  0.590420   \n",
       "2                  MNB         98.0        62.0   0.640798  0.663986   \n",
       "5              xgboost         65.0        62.0   0.574360  0.267516   \n",
       "1  k-Nearest Neighbors         62.0        61.0   0.555791  0.202397   \n",
       "\n",
       "   Tiempo transcurrido  \n",
       "4             0.097917  \n",
       "3             0.152579  \n",
       "6             0.921008  \n",
       "0            24.023519  \n",
       "2             0.005670  \n",
       "5             2.250857  \n",
       "1             0.000552  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimo resultados para countVector + n-gramas\n",
    "cv_ng_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clasificador</th>\n",
       "      <th>Prec. train</th>\n",
       "      <th>Prec. test</th>\n",
       "      <th>AUC score</th>\n",
       "      <th>F1</th>\n",
       "      <th>Tiempo transcurrido</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNB</td>\n",
       "      <td>83.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.717677</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>85.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.717032</td>\n",
       "      <td>0.657696</td>\n",
       "      <td>0.010023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.704624</td>\n",
       "      <td>0.601179</td>\n",
       "      <td>0.050988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>84.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.702807</td>\n",
       "      <td>0.637213</td>\n",
       "      <td>0.009804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>86.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.698610</td>\n",
       "      <td>0.613139</td>\n",
       "      <td>1.532785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>k-Nearest Neighbors</td>\n",
       "      <td>67.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.597685</td>\n",
       "      <td>0.342581</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>65.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.571828</td>\n",
       "      <td>0.257069</td>\n",
       "      <td>0.263801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Clasificador  Prec. train  Prec. test  AUC score        F1  \\\n",
       "2                  MNB         83.0        74.0   0.717677  0.638609   \n",
       "3      RidgeClassifier         85.0        73.0   0.717032  0.657696   \n",
       "6  Logistic Regression         82.0        73.0   0.704624  0.601179   \n",
       "4           Perceptron         84.0        72.0   0.702807  0.637213   \n",
       "0        Decision Tree         86.0        72.0   0.698610  0.613139   \n",
       "1  k-Nearest Neighbors         67.0        64.0   0.597685  0.342581   \n",
       "5              xgboost         65.0        62.0   0.571828  0.257069   \n",
       "\n",
       "   Tiempo transcurrido  \n",
       "2             0.001618  \n",
       "3             0.010023  \n",
       "6             0.050988  \n",
       "4             0.009804  \n",
       "0             1.532785  \n",
       "1             0.000510  \n",
       "5             0.263801  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimo resultados para TF-IDF + n-gramas\n",
    "tf_ng_results.sort_values(by=[\"Prec. test\", \"AUC score\"], ascending=(False,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Envío de datos\n",
    "Preparo el submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(sub_file,model,test_vector):\n",
    "    \n",
    "    '''Input- sub_file=Location of the file submission file\n",
    "              model=final fit model to be used for predictions\n",
    "              test_vector=pre-processed and vectorized test dataset\n",
    "       Output- submission file in .csv format with predictions       \n",
    "    \n",
    "    '''\n",
    "    sub_df = pd.read_csv(sub_file)\n",
    "    sub_df[\"target\"] = model.predict(test_vector)\n",
    "    sub_df.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB + countVector\n",
    "sub_file = \"/home/alejandro/Documents/75.06-Datos/data/sample_submission.csv\"\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_cv, y_train_cv)\n",
    "submission(sub_file,mnb_model,test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression +TF-IDF (1-2-gramas)\n",
    "sub_file = \"/home/alejandro/Documents/75.06-Datos/data/sample_submission.csv\"\n",
    "lr_model = LogisticRegression(C=1.0)\n",
    "lr_model.fit(x_train_tf, y_train_tf)\n",
    "submission(sub_file,lr_model,test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNB + TF-IDF (1-2-gramas)\n",
    "sub_file = \"/home/alejandro/Documents/75.06-Datos/data/sample_submission.csv\"\n",
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(x_train_tf, y_train_tf)\n",
    "submission(sub_file,mnb_model,test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
